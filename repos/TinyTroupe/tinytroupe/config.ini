[OpenAI]
#
# OpenAI or Azure OpenAI Service
#

# Default options: openai, azure, ollama
API_TYPE=openai

# Check Azure's documentation for updates here:
# https://learn.microsoft.com/en-us/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line&pivots=programming-language-python
AZURE_API_VERSION=2023-05-15

# Base URL for Ollama local server
OLLAMA_BASE_URL=http://localhost:11434/v1

#
# Models
#

# The main text generation model, used for agent responses
MODEL=gpt-4.1-mini

# Reasoning model is used when precise reasoning is required, such as when computing detailed analyses of simulation properties.
REASONING_MODEL=o3-mini

# Embedding model is used for text similarity tasks
EMBEDDING_MODEL=text-embedding-3-small 

#
# Model parameters
#
MAX_TOKENS=32000
TEMPERATURE=1.5
FREQ_PENALTY=0.1
PRESENCE_PENALTY=0.1
TIMEOUT=480
MAX_ATTEMPTS=5
WAITING_TIME=1
EXPONENTIAL_BACKOFF_FACTOR=5

REASONING_EFFORT=high

#
# Caching
#

CACHE_API_CALLS=False
CACHE_FILE_NAME=openai_api_cache.pickle

#
# Other
#

MAX_CONTENT_DISPLAY_LENGTH=4000

[Simulation]

PARALLEL_AGENT_GENERATION=True
PARALLEL_AGENT_ACTIONS=True
MAX_WORKERS=None
PARALLEL_EXECUTION_TIMEOUT=300
COLLECT_PARALLEL_METRICS=True

RAI_HARMFUL_CONTENT_PREVENTION=True
RAI_COPYRIGHT_INFRINGEMENT_PREVENTION=True

[Cognition]

ENABLE_MEMORY_CONSOLIDATION=True

MIN_EPISODE_LENGTH=15
MAX_EPISODE_LENGTH=50

EPISODIC_MEMORY_FIXED_PREFIX_LENGTH=10
EPISODIC_MEMORY_LOOKBACK_LENGTH=20

[Memory]
# Maximum size for episodic memory (number of memories)
# Set to 0 or None for unbounded memory (not recommended for long simulations)
MAX_EPISODIC_MEMORY_SIZE=1000

# Memory cleanup strategy when max size is reached
# Options: fifo (first-in-first-out), age (oldest first), relevance (least relevant first)
MEMORY_CLEANUP_STRATEGY=fifo

# Warn when memory usage reaches this percentage of max size
MEMORY_WARNING_THRESHOLD=0.8

# Enable automatic memory consolidation when threshold is reached
AUTO_CONSOLIDATE_ON_THRESHOLD=True

# Threshold for automatic consolidation (number of memories)
AUTO_CONSOLIDATION_THRESHOLD=500

[Cache]
# Maximum number of cached states in simulation cache
# Set to 0 or None for unbounded cache (not recommended)
MAX_CACHE_SIZE=10000

# Cache eviction policy when max size is reached
# Options: lru (least recently used), fifo (first-in-first-out), size (largest states first)
CACHE_EVICTION_POLICY=lru

# Warn when cache usage reaches this percentage of max size
CACHE_WARNING_THRESHOLD=0.8

# Enable cache compression for large states (reduces memory usage)
ENABLE_CACHE_COMPRESSION=False

# Minimum state size (in bytes) to compress
CACHE_COMPRESSION_THRESHOLD=10000

# Enable detailed cache analytics
COLLECT_CACHE_METRICS=True

# Semantic similarity caching (experimental)
# Enables fuzzy cache hits based on embedding similarity
ENABLE_SEMANTIC_CACHE=False

# Minimum cosine similarity for semantic cache hit (0.0-1.0)
SEMANTIC_SIMILARITY_THRESHOLD=0.85

# Maximum entries in semantic cache
MAX_SEMANTIC_CACHE_ENTRIES=1000

[ActionGenerator]
MAX_ATTEMPTS=2

# This will determine whether any of the following verifications and corrections are performed.
ENABLE_QUALITY_CHECKS=False

ENABLE_REGENERATION=True
ENABLE_DIRECT_CORRECTION=False

ENABLE_QUALITY_CHECK_FOR_PERSONA_ADHERENCE=True
ENABLE_QUALITY_CHECK_FOR_SELFCONSISTENCY=False
ENABLE_QUALITY_CHECK_FOR_FLUENCY=False
ENABLE_QUALITY_CHECK_FOR_SUITABILITY=False
ENABLE_QUALITY_CHECK_FOR_SIMILARITY=False

CONTINUE_ON_FAILURE=True

# 0 to 9
QUALITY_THRESHOLD = 5


[Logging]
LOGLEVEL=ERROR
# ERROR
# WARNING
# INFO
# DEBUG

# Structured telemetry for LLM calls
LLM_TELEMETRY_ENABLED=False
LLM_TELEMETRY_PATH=logs/llm_telemetry.jsonl

[Moderation]
# Enable OpenAI moderation checks on prompts before model calls
ENABLE_MODERATION=False
# Action when content is flagged: warn (log only) or block (return a safe stub response)
MODERATION_ACTION=warn
# Moderation model to invoke when enabled
MODERATION_MODEL=omni-moderation-latest
# Content returned to the simulation when a message is blocked
MODERATION_BLOCK_MESSAGE=[BLOCKED BY MODERATION]